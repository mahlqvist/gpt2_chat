{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Check for GPU and set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set eos_token as pad_token for the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load the model, set pad_token_id and move to device\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\t\"gpt2\", \n",
    "\tpad_token_id=tokenizer.eos_token_id,\n",
    "\ttorch_dtype=torch.float16,\n",
    "\tdevice_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(hasattr(model, \"device\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    if not hasattr(model, \"device\"):\n",
    "        return inputs\n",
    "    return {k: v.to(model.device) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, tokenizer, model):\n",
    "    inputs = prepare_inputs(prompt, tokenizer, model)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id \n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is in the hands of the next generation of researchers.\n",
      "\n",
      "\"We are in a very exciting time in AI,\" said Dr. Michael S. Hirsch, a professor of computer science at the University of California, Berkeley.\n"
     ]
    }
   ],
   "source": [
    "res = get_response(\"The future of AI is\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_sample(prompt, tokenizer, model):\n",
    "    inputs = prepare_inputs(prompt, tokenizer, model)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id   \n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is clear. A computer can be programmed to be able to learn and adapt to the world, and it can learn from a computer that can adapt itself.\n",
      "\n",
      "This is where AI comes in. The future will be much different\n"
     ]
    }
   ],
   "source": [
    "res = get_response_sample(\"The future of AI is\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_improved(prompt, tokenizer, model):\n",
    "    inputs = prepare_inputs(prompt, tokenizer, model)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        eos_token_id=tokenizer.eos_token_id   \n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is uncertain. But we're still seeing the emergence and development of tools that can be used to manage, analyze, predict, control information in real-time.\"\n",
      ": \"In this paper, I'll show you how our algorithm\n"
     ]
    }
   ],
   "source": [
    "res = get_response_improved(\"The future of AI is\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnlyAtPunctuation(StoppingCriteria):\n",
    "    def __init__(self, stop_words:list):\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Ensure clean text\n",
    "        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        suffixes = tuple(self.stop_words)\n",
    "\n",
    "        if decoded_text.endswith(suffixes):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnlyAtPunctuation(stop_words=[\".\", \"!\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_with_stop(prompt, tokenizer, model):\n",
    "    inputs = prepare_inputs(prompt, tokenizer, model)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=stopping_criteria  # Add custom stopping criteria   \n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is not yet clear, but some research suggests it may soon be possible to do so.\n"
     ]
    }
   ],
   "source": [
    "res = get_response_with_stop(\"The future of AI is\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopAtPunctuation(StoppingCriteria):\n",
    "    def __init__(self, stop_words, min_sentences=2):\n",
    "        self.stop_words = stop_words\n",
    "        self.min_sentences = min_sentences\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Ensure clean text\n",
    "        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        suffixes = tuple(self.stop_words)\n",
    "\n",
    "        # Convert list into a regex pattern\n",
    "        regex = f\"[{''.join(re.escape(word) for word in self.stop_words)}]+\"\n",
    "        matches = re.findall(regex, decoded_text)\n",
    "\n",
    "        # Count the number of complete sentences\n",
    "        sentence_count = len(matches)\n",
    "\n",
    "        if sentence_count >= self.min_sentences and decoded_text.endswith(suffixes):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "# Require at least 3 sentences before stopping\n",
    "stopping_criteria = StoppingCriteriaList([StopAtPunctuation(stop_words=[\".\", \"!\"], min_sentences=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_final(prompt, tokenizer, model):\n",
    "    inputs = prepare_inputs(prompt, tokenizer, model)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=stopping_criteria  # Add custom stopping criteria   \n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is uncertain. We may well be able to develop a machine that can help us understand and control many things, but we have yet not done it before.\"\n",
      "\n",
      "\n",
      " \"This has been my first experience with the technology,\" said Professor Lawrence Krauss in an interview from Zurich on Thursday (11 July).\n"
     ]
    }
   ],
   "source": [
    "res = get_response_final(\"The future of AI\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
