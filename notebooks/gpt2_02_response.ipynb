{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, tokenizer, model):\n",
    "    usr_query = prompt\n",
    "    inputs = tokenizer(usr_query, return_tensors=\"pt\", padding=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=128,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id \n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is uncertain. The future is not yet clear. But it is certainly possible that AI will be able to solve many of the problems that we face today.\n",
      "\n",
      "The Future of Artificial Intelligence\n",
      ". . .\n",
      " (1) The Future Of Artificial Life. (2) Artificial intelligence will become a reality. It will not be a \"new\" technology. Rather, it will evolve into a new kind of technology that will enable us to live in a world where we can live with dignity and respect for the human being. We will live a life of dignity, respect, and dignity for ourselves and for others. This\n"
     ]
    }
   ],
   "source": [
    "res = get_response(\"The future of AI is\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_sample(prompt, tokenizer, model):\n",
    "    usr_query = prompt\n",
    "    inputs = tokenizer(usr_query, return_tensors=\"pt\", padding=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=128,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id   \n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is going to be extremely exciting,\" said Hainan. \"The fact that the world is now connected to computers means that AI will be a lot easier to understand and understand than previous generations. This is why we are working on ways to make AI more like human beings.\"\n",
      "\n",
      "In other words, we're going ahead with the evolution of artificial intelligence in a way that's not just about the computing power of humans, but also about how AI can work in the real world.\n",
      ": The Future of Artificial Intelligence: What Will It Look Like?\n",
      ", a collaborative effort between Google, Facebook and the MIT Media\n"
     ]
    }
   ],
   "source": [
    "res = get_response_sample(\"The future of AI is\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_improved(prompt, tokenizer, model):\n",
    "    usr_query = prompt\n",
    "    inputs = tokenizer(usr_query, return_tensors=\"pt\", padding=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=128,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        eos_token_id=tokenizer.eos_token_id   \n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is not yet clear. There are a number and some indications that the technology will be used to solve problems such as crime, terrorism, disease or even for medical purposes.\"\n",
      "\n",
      "\n",
      " (Image: Getty Images)\n"
     ]
    }
   ],
   "source": [
    "res = get_response_improved(\"The future of AI is\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopAtPunctuation(StoppingCriteria):\n",
    "    def __init__(self, stop_words:list):\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Ensure clean text\n",
    "        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        suffixes = tuple(self.stop_words)\n",
    "\n",
    "        if decoded_text.endswith(suffixes):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopAtPunctuation(stop_words=[\".\", \"!\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_with_stop(prompt, tokenizer, model):\n",
    "    usr_query = prompt\n",
    "    inputs = tokenizer(usr_query, return_tensors=\"pt\", padding=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=stopping_criteria  # Add custom stopping criteria   \n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is in question.\n"
     ]
    }
   ],
   "source": [
    "res = get_response_with_stop(\"The future of AI is\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class StopAtPunctuation(StoppingCriteria):\n",
    "    def __init__(self, stop_words, min_sentences=2):\n",
    "        self.stop_words = stop_words\n",
    "        self.min_sentences = min_sentences\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Ensure clean text\n",
    "        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        suffixes = tuple(self.stop_words)\n",
    "\n",
    "        # Convert list into a regex pattern\n",
    "        regex = f\"[{''.join(re.escape(word) for word in self.stop_words)}]+\"\n",
    "        matches = re.findall(regex, decoded_text)\n",
    "\n",
    "        # Count the number of complete sentences\n",
    "        sentence_count = len(matches)\n",
    "        #print(f\"Sentence count: {sentence_count}\")  # Debugging print\n",
    "\n",
    "        if sentence_count >= self.min_sentences and decoded_text.endswith(suffixes):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "# Require at least 3 sentences before stopping\n",
    "stopping_criteria = StoppingCriteriaList([StopAtPunctuation(stop_words=[\".\", \"!\"], min_sentences=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_final(prompt, tokenizer, model):\n",
    "    usr_query = prompt\n",
    "    inputs = tokenizer(usr_query, return_tensors=\"pt\", padding=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=stopping_criteria  # Add custom stopping criteria   \n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is unknown, but there are a number that could be considered significant. One such possibility would probably involve the development and adoption of intelligent machines (AI), which will allow us to understand how we interact with our environment by understanding its laws or patterns in order to better tailor their behavior for human needs.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = get_response_final(\"The future of AI is\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
