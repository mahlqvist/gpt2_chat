{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "_ = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Use EOS as PAD\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "# Load gpt-2 in 8-bit to save memory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "\ttorch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"microsoft/wiki_qa\", \"rajpurkar/squad\", \"SoftAge-AI/sft-conversational_dataset\", \"D1rtyB1rd/Beloved-Everyday-Conversations\"]\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(datasets[0])\n",
    "\n",
    "# Take a small subset (to save memory)\n",
    "ds_train = ds[\"train\"].shuffle(seed=42).select(range(800))\n",
    "ds_val = ds[\"validation\"].shuffle(seed=42).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove unwanted spaces before punctuation\n",
    "    text = re.sub(r'\\s+([.,;:!?%)])', r'\\1', text)\n",
    "    # Replace multiple punctuations with a single \n",
    "    text = re.sub(r'([.,;:!?%])\\1+', r'\\1', text)\n",
    "    # Replace multiple spaces between words with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Ensure no leading or trailing spaces\n",
    "    text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_id': 'Q473', 'question': 'how much does united states spend on health care', 'document_title': 'Health care in the United States', 'answer': 'The U.S. Census Bureau reported that 49.9 million residents, 16.3% of the population, were uninsured in 2010 (up from 49.0 million residents, 16.1% of the population, in 2009).', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# Look at the structure\n",
    "print(ds_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4d6c53c51e4f44ade21e16d517ef8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2163b5e81fe2458bb447a4ec0d6ad83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_qa(examples):\n",
    "    texts = []\n",
    "    for q, a in zip(examples[\"question\"], examples[\"answer\"]):\n",
    "        # Format: \"User: [Question] Assistant: [Answer]\"\n",
    "        if \"?\" in q:\n",
    "            text = f\"Human: {clean_text(q.capitalize())} Bot: {clean_text(a)}{tokenizer.eos_token}\"\n",
    "        else:\n",
    "            text = f\"Human: {clean_text(q.capitalize())}? Bot: {clean_text(a)}{tokenizer.eos_token}\"\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "train_dataset = ds_train.map(format_qa, batched=True)\n",
    "val_dataset = ds_val.map(format_qa, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_id': 'Q473', 'question': 'how much does united states spend on health care', 'document_title': 'Health care in the United States', 'answer': 'The U.S. Census Bureau reported that 49.9 million residents, 16.3% of the population, were uninsured in 2010 (up from 49.0 million residents, 16.1% of the population, in 2009).', 'label': 0, 'text': 'Human: How much does united states spend on health care? Bot: The U.S. Census Bureau reported that 49.9 million residents, 16.3% of the population, were uninsured in 2010 (up from 49.0 million residents, 16.1% of the population, in 2009).<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8cb7b0da484ce0af8c0f208a4a5c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd994d885e64f62be73bbfb91744d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the datasets\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    ) \n",
    "    return tokens\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"question_id\", \"question\", \"document_title\", \"answer\", \"label\", \"text\"])\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"question_id\", \"question\", \"document_title\", \"answer\", \"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [20490, 25, 1374, 881, 857, 16503, 2585, 4341, 319, 1535, 1337, 30, 18579, 25, 383, 471, 13, 50, 13, 20962, 9840, 2098, 326, 5125, 13, 24, 1510, 5085, 11, 1467, 13, 18, 4, 286, 262, 3265, 11, 547, 32736, 287, 3050, 357, 929, 422, 5125, 13, 15, 1510, 5085, 11, 1467, 13, 16, 4, 286, 262, 3265, 11, 287, 3717, 737, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
