{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, tokenizer, model):\n",
    "    usr_query = prompt\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(usr_query, return_tensors=\"pt\", padding=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        no_repeat_ngram_size=2,\n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Articial Intelligence is in the hands of the people.\n",
      "\n",
      "The people are the ones who are going to decide what is best for the future. They are not the only ones. The people have the power to change the world. And they are doing it by the millions.\n"
     ]
    }
   ],
   "source": [
    "res = get_response(\"The future of Articial Intelligence\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_updated(prompt, tokenizer, model):\n",
    "    usr_query = prompt\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(usr_query, return_tensors=\"pt\", padding=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,  \n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Articial Intelligence is at stake.\n",
      "\n",
      "The next time you see a company trying to get into the art world, you may be wondering what's going on with their management. I'm sure they're all in the same boat as their investors. It's just that they don't know how to do it. The future is always in their hands. They have a lot of other things to think about. For instance, they can get a few years of a good deal on their\n"
     ]
    }
   ],
   "source": [
    "res = get_response_updated(\"The future of Articial Intelligence\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_final(prompt, tokenizer, model):\n",
    "    usr_query = prompt\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(usr_query, return_tensors=\"pt\", padding=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1, \n",
    "        eos_token_id=tokenizer.eos_token_id   \n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Articial Intelligence is in doubt, and the only way to stop it from becoming a reality will be through an unprecedented level-headed approach that includes both scientific research (as well as public relations) on issues such for example education policy.\n",
      "I hope this article has been useful at all levels - especially when you're trying so hard to keep up with new technology like Facebook's AI platform!\n"
     ]
    }
   ],
   "source": [
    "res = get_response_final(\"The future of Articial Intelligence\", tokenizer, model)\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
